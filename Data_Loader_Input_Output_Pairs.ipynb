{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a short story as text sample into Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The print command prints the total number of characters followed by the first 100\n",
    "characters of this file for illustration purposes. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Our goal is to tokenize this 20,479-character short story into individual words and special\n",
    "characters that we can then turn into embeddings for LLM training  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that it's common to process millions of articles and hundreds of thousands of\n",
    "books -- many gigabytes of text -- when working with LLMs. However, for educational\n",
    "purposes, it's sufficient to work with smaller text samples like a single book to\n",
    "illustrate the main ideas behind the text processing steps and to make it possible to\n",
    "run it in reasonable time on consumer hardware. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "How can we best split this text to obtain a list of tokens? For this, we go on a small\n",
    "excursion and use Python's regular expression library re for illustration purposes. (Note\n",
    "that you don't have to learn or memorize any regular expression syntax since we will\n",
    "transition to a pre-built tokenizer later in this chapter.) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Using some simple example text, we can use the re.split command with the following\n",
    "syntax to split a text on whitespace characters:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The result is a list of individual words, whitespaces, and punctuation characters:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Let's modify the regular expression splits on whitespaces (\\s) and commas, and periods\n",
    "([,.]):</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "We can see that the words and punctuation characters are now separate list entries just as\n",
    "we wanted\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "A small remaining issue is that the list still includes whitespace characters. Optionally, we\n",
    "can remove these redundant characters safely as follows:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "REMOVING WHITESPACES OR NOT\n",
    "\n",
    "\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as\n",
    "separate characters or just remove them depends on our application and its\n",
    "requirements. Removing whitespaces reduces the memory and computing\n",
    "requirements. However, keeping whitespaces can be useful if we train models that\n",
    "are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
    "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
    "that includes whitespaces.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The tokenization scheme we devised above works well on the simple sample text. Let's\n",
    "modify it a bit further so that it can also handle other types of punctuation, such as\n",
    "question marks, quotation marks, and the double-dashes we have seen earlier in the first\n",
    "100 characters of Edith Wharton's short story, along with additional special characters: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now that we got a basic tokenizer working, let's apply it to Edith Wharton's entire short\n",
    "story:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In the previous section, we tokenized Edith Wharton's short story and assigned it to a\n",
    "Python variable called preprocessed. Let's now create a list of all unique tokens and sort\n",
    "them alphabetically to determine the vocabulary size:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "After determining that the vocabulary size is 1,130 via the above code, we create the\n",
    "vocabulary and print its first 51 entries for illustration purposes:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, based on the output above, the dictionary contains individual tokens\n",
    "associated with unique integer labels. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Later in this book, when we want to convert the outputs of an LLM from numbers back into\n",
    "text, we also need a way to turn token IDs into text. \n",
    "\n",
    "For this, we can create an inverse\n",
    "version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits\n",
    "text into tokens and carries out the string-to-integer mapping to produce token IDs via the\n",
    "vocabulary. \n",
    "\n",
    "In addition, we implement a decode method that carries out the reverse\n",
    "integer-to-string mapping to convert the token IDs back into text.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "    \n",
    "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "Step 3: Process input text into token IDs\n",
    "\n",
    "Step 4: Convert token IDs back into text\n",
    "\n",
    "Step 5: Replace spaces before the specified punctuation\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a\n",
    "passage from Edith Wharton's short story to try it out in practice:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The code above prints the following token IDs:\n",
    "Next, let's see if we can turn these token IDs back into text using the decode method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on the output above, we can see that the decode method successfully converted the\n",
    "token IDs back into the original text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing\n",
    "text based on a snippet from the training set. \n",
    "\n",
    "Let's now apply it to a new text sample that\n",
    "is not contained in the training set:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     11\u001b[0m ]\n\u001b[1;32m---> 12\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     11\u001b[0m ]\n\u001b[1;32m---> 12\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The problem is that the word \"Hello\" was not used in the The Verdict short story. \n",
    "\n",
    "Hence, it\n",
    "is not contained in the vocabulary. \n",
    "\n",
    "This highlights the need to consider large and diverse\n",
    "training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
    "from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
    "<|endoftext|>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can modify the tokenizer to use an <|unk|> token if it\n",
    "encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between\n",
    "unrelated texts. \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent\n",
    "documents or books, it is common to insert a token before each document or book that\n",
    "follows a previous text source\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now modify the vocabulary to include these two special tokens, <unk> and\n",
    "<|endoftext|>, by adding these to the list of all unique words that we created in the\n",
    "previous section:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on the output of the print statement above, the new vocabulary size is 1132 (the\n",
    "vocabulary size in the previous section was 1130).\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As an additional quick check, let's print the last 5 entries of the updated vocabulary:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "A simple text tokenizer that handles unknown words</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Replace unknown words by <|unk|> tokens\n",
    "    \n",
    "Step 2: Replace spaces before the specified punctuations\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Based on comparing the de-tokenized text above with the original input text, we know that\n",
    "the training dataset, Edith Wharton's short story The Verdict, did not contain the words\n",
    "\"Hello\" and \"palace.\"\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "So far, we have discussed tokenization as an essential step in processing text as input to\n",
    "LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
    "as the following:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
    "signifies to the LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
    "and is especially useful when concatenating multiple unrelated texts,\n",
    "similar to <|endoftext|>. For instance, when combining two different\n",
    "Wikipedia articles or books, the [EOS] token indicates where one article\n",
    "ends and the next one begins.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
    "the batch might contain texts of varying lengths. To ensure all texts have\n",
    "the same length, the shorter texts are extended or \"padded\" using the\n",
    "[PAD] token, up to the length of the longest text in the batch.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
    "above but only uses an <|endoftext|> token for simplicity\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
    "down words into subword units\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BYTE PAIR ENCODING (BPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We implemented a simple tokenization scheme in the previous sections for illustration\n",
    "purposes. \n",
    "\n",
    "This section covers a more sophisticated tokenization scheme based on a concept\n",
    "called byte pair encoding (BPE). \n",
    "\n",
    "The BPE tokenizer covered in this section was used to train\n",
    "LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Since implementing BPE can be relatively complicated, we will use an existing Python\n",
    "open-source library called tiktoken (https://github.com/openai/tiktoken). \n",
    "\n",
    "This library implements\n",
    "the BPE algorithm very efficiently based on source code in Rust.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\welcome\\desktop\\doomscroll\\.venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\welcome\\desktop\\doomscroll\\.venv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\welcome\\desktop\\doomscroll\\.venv\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\welcome\\desktop\\doomscroll\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\welcome\\desktop\\doomscroll\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\welcome\\desktop\\doomscroll\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\welcome\\desktop\\doomscroll\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via\n",
    "an encode method:</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The code above prints the following token IDs:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We can then convert the token IDs back into text using the decode method, similar to our\n",
    "SimpleTokenizerV2 earlier:</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We can make two noteworthy observations based on the token IDs and decoded text\n",
    "above. \n",
    "\n",
    "First, the <|endoftext|> token is assigned a relatively large token ID, namely,\n",
    "50256. \n",
    "\n",
    "In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3,\n",
    "and the original model used in ChatGPT, has a total vocabulary size of 50,257, with\n",
    "<|endoftext|> being assigned the largest token ID.\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Second, the BPE tokenizer above encodes and decodes unknown words, such as\n",
    "\"someunknownPlace\" correctly. \n",
    "\n",
    "The BPE tokenizer can handle any unknown word. How does\n",
    "it achieve this without using <|unk|> tokens?\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary\n",
    "into smaller subword units or even individual characters.\n",
    "\n",
    "The enables it to handle out-ofvocabulary words. \n",
    "\n",
    "So, thanks to the BPE algorithm, if the tokenizer encounters an\n",
    "unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or\n",
    "characters\n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us take another simple example to illustrate how the BPE tokenizer deals with unknown tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING INPUT-TARGET PAIRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To get started, we will first tokenize the whole The Verdict short story we worked with\n",
    "earlier using the BPE tokenizer introduced in the previous section:</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Executing the code above will return 5145, the total number of tokens in the training set,\n",
    "after applying the BPE tokenizer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it\n",
    "results in a slightly more interesting text passage in the next steps:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens\n",
    "and y contains the targets, which are the inputs shifted by 1:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The context size determines how many tokens are included in the input\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of the input\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) \n",
    "#to predict the next word in the sequence. \n",
    "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Processing the inputs along with the targets, which are the inputs shifted by one position,\n",
    "we can then create the next-word prediction tasks as\n",
    "follows:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token\n",
    "ID on the right side of the arrow represents the target token ID that the LLM is supposed to\n",
    "predict.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For illustration purposes, let's repeat the previous code but convert the token IDs into\n",
    "text:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "We've now created the input-target pairs that we can turn into use for the LLM training in\n",
    "upcoming chapters.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that\n",
    "iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which\n",
    "can be thought of as multidimensional arrays.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In particular, we are interested in returning two tensors: an input tensor containing the\n",
    "text that the LLM sees and a target tensor that includes the targets for the LLM to predict,\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING A DATA LOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and\n",
    "DataLoader classes.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Tokenize the entire text\n",
    "    \n",
    "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "\n",
    "Step 3: Return the total number of rows in the dataset\n",
    "\n",
    "Step 4: Return a single row from the dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp310-cp310-win_amd64.whl.metadata (29 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\welcome\\desktop\\doomscroll\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading torch-2.7.0-cp310-cp310-win_amd64.whl (212.5 MB)\n",
      "   ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/212.5 MB 5.6 MB/s eta 0:00:38\n",
      "   ---------------------------------------- 1.8/212.5 MB 4.6 MB/s eta 0:00:47\n",
      "   ---------------------------------------- 2.6/212.5 MB 4.3 MB/s eta 0:00:49\n",
      "    --------------------------------------- 3.4/212.5 MB 4.2 MB/s eta 0:00:50\n",
      "    --------------------------------------- 4.2/212.5 MB 4.1 MB/s eta 0:00:51\n",
      "    --------------------------------------- 5.0/212.5 MB 4.1 MB/s eta 0:00:51\n",
      "   - -------------------------------------- 5.8/212.5 MB 4.1 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 6.3/212.5 MB 3.9 MB/s eta 0:00:54\n",
      "   - -------------------------------------- 7.3/212.5 MB 4.0 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 8.1/212.5 MB 4.0 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 8.9/212.5 MB 4.0 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 9.7/212.5 MB 4.0 MB/s eta 0:00:52\n",
      "   - -------------------------------------- 10.5/212.5 MB 4.0 MB/s eta 0:00:51\n",
      "   -- ------------------------------------- 11.3/212.5 MB 4.0 MB/s eta 0:00:51\n",
      "   -- ------------------------------------- 12.1/212.5 MB 4.0 MB/s eta 0:00:51\n",
      "   -- ------------------------------------- 12.8/212.5 MB 3.9 MB/s eta 0:00:51\n",
      "   -- ------------------------------------- 13.6/212.5 MB 3.9 MB/s eta 0:00:51\n",
      "   -- ------------------------------------- 14.4/212.5 MB 4.0 MB/s eta 0:00:51\n",
      "   -- ------------------------------------- 15.2/212.5 MB 4.0 MB/s eta 0:00:50\n",
      "   --- ------------------------------------ 16.0/212.5 MB 3.9 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 16.8/212.5 MB 3.9 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 17.3/212.5 MB 3.9 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 18.1/212.5 MB 3.9 MB/s eta 0:00:51\n",
      "   --- ------------------------------------ 18.9/212.5 MB 3.9 MB/s eta 0:00:50\n",
      "   --- ------------------------------------ 19.7/212.5 MB 3.9 MB/s eta 0:00:50\n",
      "   --- ------------------------------------ 20.4/212.5 MB 3.9 MB/s eta 0:00:50\n",
      "   --- ------------------------------------ 21.2/212.5 MB 3.9 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 21.8/212.5 MB 3.9 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 22.5/212.5 MB 3.9 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 23.3/212.5 MB 3.8 MB/s eta 0:00:50\n",
      "   ---- ----------------------------------- 24.1/212.5 MB 3.8 MB/s eta 0:00:49\n",
      "   ---- ----------------------------------- 24.9/212.5 MB 3.8 MB/s eta 0:00:49\n",
      "   ---- ----------------------------------- 25.7/212.5 MB 3.8 MB/s eta 0:00:49\n",
      "   ---- ----------------------------------- 26.5/212.5 MB 3.8 MB/s eta 0:00:49\n",
      "   ----- ---------------------------------- 27.3/212.5 MB 3.8 MB/s eta 0:00:49\n",
      "   ----- ---------------------------------- 28.0/212.5 MB 3.8 MB/s eta 0:00:49\n",
      "   ----- ---------------------------------- 28.8/212.5 MB 3.8 MB/s eta 0:00:48\n",
      "   ----- ---------------------------------- 29.6/212.5 MB 3.8 MB/s eta 0:00:48\n",
      "   ----- ---------------------------------- 30.4/212.5 MB 3.8 MB/s eta 0:00:48\n",
      "   ----- ---------------------------------- 31.2/212.5 MB 3.8 MB/s eta 0:00:48\n",
      "   ------ --------------------------------- 32.0/212.5 MB 3.8 MB/s eta 0:00:47\n",
      "   ------ --------------------------------- 32.8/212.5 MB 3.9 MB/s eta 0:00:47\n",
      "   ------ --------------------------------- 33.6/212.5 MB 3.9 MB/s eta 0:00:47\n",
      "   ------ --------------------------------- 34.3/212.5 MB 3.9 MB/s eta 0:00:47\n",
      "   ------ --------------------------------- 35.1/212.5 MB 3.9 MB/s eta 0:00:47\n",
      "   ------ --------------------------------- 35.9/212.5 MB 3.9 MB/s eta 0:00:46\n",
      "   ------ --------------------------------- 36.7/212.5 MB 3.9 MB/s eta 0:00:46\n",
      "   ------- -------------------------------- 37.5/212.5 MB 3.9 MB/s eta 0:00:46\n",
      "   ------- -------------------------------- 38.3/212.5 MB 3.9 MB/s eta 0:00:46\n",
      "   ------- -------------------------------- 39.1/212.5 MB 3.9 MB/s eta 0:00:45\n",
      "   ------- -------------------------------- 39.8/212.5 MB 3.9 MB/s eta 0:00:45\n",
      "   ------- -------------------------------- 40.6/212.5 MB 3.9 MB/s eta 0:00:45\n",
      "   ------- -------------------------------- 41.4/212.5 MB 3.9 MB/s eta 0:00:45\n",
      "   ------- -------------------------------- 42.2/212.5 MB 3.9 MB/s eta 0:00:45\n",
      "   -------- ------------------------------- 43.0/212.5 MB 3.9 MB/s eta 0:00:44\n",
      "   -------- ------------------------------- 43.8/212.5 MB 3.9 MB/s eta 0:00:44\n",
      "   -------- ------------------------------- 44.6/212.5 MB 3.9 MB/s eta 0:00:44\n",
      "   -------- ------------------------------- 45.4/212.5 MB 3.9 MB/s eta 0:00:44\n",
      "   -------- ------------------------------- 46.1/212.5 MB 3.9 MB/s eta 0:00:44\n",
      "   -------- ------------------------------- 46.9/212.5 MB 3.9 MB/s eta 0:00:43\n",
      "   -------- ------------------------------- 47.7/212.5 MB 3.9 MB/s eta 0:00:43\n",
      "   --------- ------------------------------ 48.5/212.5 MB 3.9 MB/s eta 0:00:43\n",
      "   --------- ------------------------------ 49.3/212.5 MB 3.8 MB/s eta 0:00:43\n",
      "   --------- ------------------------------ 49.8/212.5 MB 3.8 MB/s eta 0:00:43\n",
      "   --------- ------------------------------ 50.6/212.5 MB 3.8 MB/s eta 0:00:43\n",
      "   --------- ------------------------------ 51.4/212.5 MB 3.8 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 52.2/212.5 MB 3.8 MB/s eta 0:00:42\n",
      "   --------- ------------------------------ 53.0/212.5 MB 3.8 MB/s eta 0:00:42\n",
      "   ---------- ----------------------------- 53.7/212.5 MB 3.8 MB/s eta 0:00:42\n",
      "   ---------- ----------------------------- 54.5/212.5 MB 3.8 MB/s eta 0:00:42\n",
      "   ---------- ----------------------------- 55.3/212.5 MB 3.8 MB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 56.1/212.5 MB 3.8 MB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 56.9/212.5 MB 3.8 MB/s eta 0:00:41\n",
      "   ---------- ----------------------------- 57.7/212.5 MB 3.8 MB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 58.5/212.5 MB 3.8 MB/s eta 0:00:41\n",
      "   ----------- ---------------------------- 59.2/212.5 MB 3.8 MB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 60.0/212.5 MB 3.8 MB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 60.8/212.5 MB 3.8 MB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 61.6/212.5 MB 3.8 MB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 62.4/212.5 MB 3.8 MB/s eta 0:00:40\n",
      "   ----------- ---------------------------- 63.2/212.5 MB 3.8 MB/s eta 0:00:39\n",
      "   ------------ --------------------------- 64.0/212.5 MB 3.8 MB/s eta 0:00:39\n",
      "   ------------ --------------------------- 64.7/212.5 MB 3.8 MB/s eta 0:00:39\n",
      "   ------------ --------------------------- 65.5/212.5 MB 3.8 MB/s eta 0:00:39\n",
      "   ------------ --------------------------- 66.3/212.5 MB 3.8 MB/s eta 0:00:39\n",
      "   ------------ --------------------------- 67.1/212.5 MB 3.8 MB/s eta 0:00:38\n",
      "   ------------ --------------------------- 67.6/212.5 MB 3.8 MB/s eta 0:00:38\n",
      "   ------------ --------------------------- 68.4/212.5 MB 3.8 MB/s eta 0:00:38\n",
      "   ------------- -------------------------- 69.2/212.5 MB 3.8 MB/s eta 0:00:38\n",
      "   ------------- -------------------------- 70.0/212.5 MB 3.8 MB/s eta 0:00:38\n",
      "   ------------- -------------------------- 70.8/212.5 MB 3.8 MB/s eta 0:00:37\n",
      "   ------------- -------------------------- 71.6/212.5 MB 3.8 MB/s eta 0:00:37\n",
      "   ------------- -------------------------- 72.4/212.5 MB 3.8 MB/s eta 0:00:37\n",
      "   ------------- -------------------------- 73.1/212.5 MB 3.8 MB/s eta 0:00:37\n",
      "   ------------- -------------------------- 73.9/212.5 MB 3.8 MB/s eta 0:00:37\n",
      "   -------------- ------------------------- 74.7/212.5 MB 3.8 MB/s eta 0:00:36\n",
      "   -------------- ------------------------- 75.5/212.5 MB 3.8 MB/s eta 0:00:36\n",
      "   -------------- ------------------------- 76.3/212.5 MB 3.8 MB/s eta 0:00:36\n",
      "   -------------- ------------------------- 77.1/212.5 MB 3.8 MB/s eta 0:00:36\n",
      "   -------------- ------------------------- 77.6/212.5 MB 3.8 MB/s eta 0:00:36\n",
      "   -------------- ------------------------- 78.4/212.5 MB 3.8 MB/s eta 0:00:35\n",
      "   -------------- ------------------------- 79.2/212.5 MB 3.8 MB/s eta 0:00:35\n",
      "   --------------- ------------------------ 80.0/212.5 MB 3.8 MB/s eta 0:00:35\n",
      "   --------------- ------------------------ 80.7/212.5 MB 3.8 MB/s eta 0:00:35\n",
      "   --------------- ------------------------ 81.5/212.5 MB 3.8 MB/s eta 0:00:35\n",
      "   --------------- ------------------------ 82.3/212.5 MB 3.8 MB/s eta 0:00:34\n",
      "   --------------- ------------------------ 83.1/212.5 MB 3.8 MB/s eta 0:00:34\n",
      "   --------------- ------------------------ 83.9/212.5 MB 3.8 MB/s eta 0:00:34\n",
      "   --------------- ------------------------ 84.7/212.5 MB 3.8 MB/s eta 0:00:34\n",
      "   ---------------- ----------------------- 85.5/212.5 MB 3.8 MB/s eta 0:00:34\n",
      "   ---------------- ----------------------- 86.2/212.5 MB 3.8 MB/s eta 0:00:33\n",
      "   ---------------- ----------------------- 87.0/212.5 MB 3.8 MB/s eta 0:00:33\n",
      "   ---------------- ----------------------- 87.8/212.5 MB 3.8 MB/s eta 0:00:33\n",
      "   ---------------- ----------------------- 88.6/212.5 MB 3.8 MB/s eta 0:00:33\n",
      "   ---------------- ----------------------- 89.4/212.5 MB 3.8 MB/s eta 0:00:33\n",
      "   ---------------- ----------------------- 90.2/212.5 MB 3.8 MB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 91.0/212.5 MB 3.8 MB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 91.8/212.5 MB 3.8 MB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 92.5/212.5 MB 3.8 MB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 93.3/212.5 MB 3.8 MB/s eta 0:00:32\n",
      "   ----------------- ---------------------- 93.8/212.5 MB 3.8 MB/s eta 0:00:31\n",
      "   ----------------- ---------------------- 94.6/212.5 MB 3.8 MB/s eta 0:00:31\n",
      "   ----------------- ---------------------- 95.4/212.5 MB 3.8 MB/s eta 0:00:31\n",
      "   ------------------ --------------------- 96.2/212.5 MB 3.8 MB/s eta 0:00:31\n",
      "   ------------------ --------------------- 97.0/212.5 MB 3.8 MB/s eta 0:00:31\n",
      "   ------------------ --------------------- 97.8/212.5 MB 3.8 MB/s eta 0:00:30\n",
      "   ------------------ --------------------- 98.8/212.5 MB 3.8 MB/s eta 0:00:30\n",
      "   ------------------ --------------------- 99.4/212.5 MB 3.8 MB/s eta 0:00:30\n",
      "   ------------------ --------------------- 100.1/212.5 MB 3.8 MB/s eta 0:00:30\n",
      "   ------------------- -------------------- 101.2/212.5 MB 3.8 MB/s eta 0:00:30\n",
      "   ------------------- -------------------- 101.7/212.5 MB 3.8 MB/s eta 0:00:29\n",
      "   ------------------- -------------------- 102.5/212.5 MB 3.8 MB/s eta 0:00:29\n",
      "   ------------------- -------------------- 103.3/212.5 MB 3.8 MB/s eta 0:00:29\n",
      "   ------------------- -------------------- 104.1/212.5 MB 3.8 MB/s eta 0:00:29\n",
      "   ------------------- -------------------- 104.9/212.5 MB 3.8 MB/s eta 0:00:29\n",
      "   ------------------- -------------------- 105.6/212.5 MB 3.8 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 106.4/212.5 MB 3.8 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 107.2/212.5 MB 3.8 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 108.0/212.5 MB 3.8 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 108.8/212.5 MB 3.8 MB/s eta 0:00:28\n",
      "   -------------------- ------------------- 109.6/212.5 MB 3.8 MB/s eta 0:00:27\n",
      "   -------------------- ------------------- 110.4/212.5 MB 3.8 MB/s eta 0:00:27\n",
      "   -------------------- ------------------- 111.1/212.5 MB 3.8 MB/s eta 0:00:27\n",
      "   --------------------- ------------------ 111.9/212.5 MB 3.8 MB/s eta 0:00:27\n",
      "   --------------------- ------------------ 112.7/212.5 MB 3.8 MB/s eta 0:00:27\n",
      "   --------------------- ------------------ 113.5/212.5 MB 3.8 MB/s eta 0:00:26\n",
      "   --------------------- ------------------ 114.3/212.5 MB 3.8 MB/s eta 0:00:26\n",
      "   --------------------- ------------------ 115.1/212.5 MB 3.8 MB/s eta 0:00:26\n",
      "   --------------------- ------------------ 115.9/212.5 MB 3.8 MB/s eta 0:00:26\n",
      "   --------------------- ------------------ 116.7/212.5 MB 3.8 MB/s eta 0:00:25\n",
      "   ---------------------- ----------------- 117.4/212.5 MB 3.8 MB/s eta 0:00:25\n",
      "   ---------------------- ----------------- 118.2/212.5 MB 3.8 MB/s eta 0:00:25\n",
      "   ---------------------- ----------------- 119.0/212.5 MB 3.8 MB/s eta 0:00:25\n",
      "   ---------------------- ----------------- 119.8/212.5 MB 3.8 MB/s eta 0:00:25\n",
      "   ---------------------- ----------------- 120.6/212.5 MB 3.8 MB/s eta 0:00:25\n",
      "   ---------------------- ----------------- 121.4/212.5 MB 3.8 MB/s eta 0:00:24\n",
      "   ---------------------- ----------------- 122.2/212.5 MB 3.8 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 122.9/212.5 MB 3.8 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 123.5/212.5 MB 3.8 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 124.3/212.5 MB 3.8 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 125.0/212.5 MB 3.8 MB/s eta 0:00:23\n",
      "   ----------------------- ---------------- 125.8/212.5 MB 3.8 MB/s eta 0:00:23\n",
      "   ----------------------- ---------------- 126.6/212.5 MB 3.8 MB/s eta 0:00:23\n",
      "   ----------------------- ---------------- 127.4/212.5 MB 3.8 MB/s eta 0:00:23\n",
      "   ------------------------ --------------- 128.2/212.5 MB 3.8 MB/s eta 0:00:23\n",
      "   ------------------------ --------------- 129.0/212.5 MB 3.8 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 129.8/212.5 MB 3.8 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 130.5/212.5 MB 3.8 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 131.3/212.5 MB 3.8 MB/s eta 0:00:22\n",
      "   ------------------------ --------------- 132.1/212.5 MB 3.8 MB/s eta 0:00:22\n",
      "   ------------------------- -------------- 132.9/212.5 MB 3.8 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 133.7/212.5 MB 3.8 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 134.5/212.5 MB 3.8 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 135.0/212.5 MB 3.8 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 135.8/212.5 MB 3.8 MB/s eta 0:00:21\n",
      "   ------------------------- -------------- 136.6/212.5 MB 3.8 MB/s eta 0:00:20\n",
      "   ------------------------- -------------- 137.4/212.5 MB 3.8 MB/s eta 0:00:20\n",
      "   -------------------------- ------------- 138.1/212.5 MB 3.8 MB/s eta 0:00:20\n",
      "   -------------------------- ------------- 138.9/212.5 MB 3.8 MB/s eta 0:00:20\n",
      "   -------------------------- ------------- 139.7/212.5 MB 3.8 MB/s eta 0:00:20\n",
      "   -------------------------- ------------- 140.5/212.5 MB 3.8 MB/s eta 0:00:19\n",
      "   -------------------------- ------------- 141.3/212.5 MB 3.8 MB/s eta 0:00:19\n",
      "   -------------------------- ------------- 142.1/212.5 MB 3.8 MB/s eta 0:00:19\n",
      "   -------------------------- ------------- 142.9/212.5 MB 3.8 MB/s eta 0:00:19\n",
      "   --------------------------- ------------ 143.7/212.5 MB 3.8 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 144.4/212.5 MB 3.8 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 145.2/212.5 MB 3.8 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 146.0/212.5 MB 3.8 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 146.8/212.5 MB 3.8 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 147.6/212.5 MB 3.8 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 148.4/212.5 MB 3.8 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 149.2/212.5 MB 3.8 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 149.9/212.5 MB 3.8 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 150.7/212.5 MB 3.8 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 151.5/212.5 MB 3.8 MB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 152.3/212.5 MB 3.8 MB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 153.1/212.5 MB 3.8 MB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 153.9/212.5 MB 3.8 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 154.4/212.5 MB 3.8 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 155.5/212.5 MB 3.8 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 156.2/212.5 MB 3.8 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 157.0/212.5 MB 3.8 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 157.8/212.5 MB 3.8 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 158.6/212.5 MB 3.8 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 159.4/212.5 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 160.2/212.5 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 161.0/212.5 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 161.7/212.5 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 162.5/212.5 MB 3.8 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 163.1/212.5 MB 3.8 MB/s eta 0:00:13\n",
      "   ------------------------------ --------- 163.8/212.5 MB 3.8 MB/s eta 0:00:13\n",
      "   ------------------------------ --------- 164.6/212.5 MB 3.8 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 165.4/212.5 MB 3.8 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 166.2/212.5 MB 3.8 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 167.0/212.5 MB 3.8 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 167.8/212.5 MB 3.8 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 168.6/212.5 MB 3.8 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 169.3/212.5 MB 3.8 MB/s eta 0:00:12\n",
      "   -------------------------------- ------- 170.1/212.5 MB 3.8 MB/s eta 0:00:12\n",
      "   -------------------------------- ------- 171.2/212.5 MB 3.8 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 172.0/212.5 MB 3.8 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 172.5/212.5 MB 3.8 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 173.3/212.5 MB 3.8 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 174.1/212.5 MB 3.8 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 174.9/212.5 MB 3.8 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 175.6/212.5 MB 3.8 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 176.4/212.5 MB 3.8 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 177.2/212.5 MB 3.8 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 178.0/212.5 MB 3.8 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 178.8/212.5 MB 3.8 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 179.6/212.5 MB 3.8 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 180.4/212.5 MB 3.8 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 181.1/212.5 MB 3.8 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 181.9/212.5 MB 3.8 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 182.7/212.5 MB 3.8 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 183.5/212.5 MB 3.8 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 184.3/212.5 MB 3.8 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 185.1/212.5 MB 3.8 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 185.9/212.5 MB 3.8 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 186.6/212.5 MB 3.8 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 187.4/212.5 MB 3.8 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 188.2/212.5 MB 3.8 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 188.7/212.5 MB 3.8 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 189.5/212.5 MB 3.8 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 190.3/212.5 MB 3.8 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 191.1/212.5 MB 3.8 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 191.9/212.5 MB 3.8 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 192.7/212.5 MB 3.8 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 193.5/212.5 MB 3.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 194.2/212.5 MB 3.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 195.0/212.5 MB 3.8 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 195.8/212.5 MB 3.8 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 196.6/212.5 MB 3.8 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 197.4/212.5 MB 3.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 198.2/212.5 MB 3.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 199.0/212.5 MB 3.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 199.8/212.5 MB 3.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 200.5/212.5 MB 3.8 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 201.3/212.5 MB 3.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 202.1/212.5 MB 3.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 202.9/212.5 MB 3.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 203.7/212.5 MB 3.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 204.5/212.5 MB 3.8 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 205.3/212.5 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 206.0/212.5 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 206.8/212.5 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  207.6/212.5 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  208.4/212.5 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  209.2/212.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  210.0/212.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  210.8/212.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  211.6/212.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  212.3/212.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  212.3/212.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 212.5/212.5 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.0/6.3 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 4.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.3 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.9 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "\n",
      "   ---------------------------------------- 0/8 [mpmath]\n",
      "   ---------------------------------------- 0/8 [mpmath]\n",
      "   ---------------------------------------- 0/8 [mpmath]\n",
      "   ---------------------------------------- 0/8 [mpmath]\n",
      "   ---------------------------------------- 0/8 [mpmath]\n",
      "   ---------------------------------------- 0/8 [mpmath]\n",
      "   ---------------------------------------- 0/8 [mpmath]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ----- ---------------------------------- 1/8 [sympy]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   ---------- ----------------------------- 2/8 [networkx]\n",
      "   -------------------- ------------------- 4/8 [fsspec]\n",
      "   -------------------- ------------------- 4/8 [fsspec]\n",
      "   -------------------- ------------------- 4/8 [fsspec]\n",
      "   -------------------- ------------------- 4/8 [fsspec]\n",
      "   -------------------- ------------------- 4/8 [fsspec]\n",
      "   ------------------------------ --------- 6/8 [jinja2]\n",
      "   ------------------------------ --------- 6/8 [jinja2]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ----------------------------------- ---- 7/8 [torch]\n",
      "   ---------------------------------------- 8/8 [torch]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.3.2 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.14.0 torch-2.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.2.5-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Downloading numpy-2.2.5-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.9 MB 5.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.8/12.9 MB 4.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.6/12.9 MB 4.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.4/12.9 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 4.2/12.9 MB 3.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 5.0/12.9 MB 4.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 5.8/12.9 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.6/12.9 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.8/12.9 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 8.1/12.9 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.9/12.9 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.7/12.9 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.5/12.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.3/12.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip3 install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The GPTDatasetV1 class in listing 2.5 is based on the PyTorch Dataset class.\n",
    "\n",
    "It defines how individual rows are fetched from the dataset. \n",
    "\n",
    "Each row consists of a number of\n",
    "token IDs (based on a max_length) assigned to an input_chunk tensor. \n",
    "\n",
    "The target_chunk\n",
    "tensor contains the corresponding targets. \n",
    "\n",
    "I recommend reading on to see how the data\n",
    "returned from this dataset looks like when we combine the dataset with a PyTorch\n",
    "DataLoader -- this will bring additional intuition and clarity.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The following code will use the GPTDatasetV1 to load the inputs in batches via a PyTorch\n",
    "DataLoader:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Step 1: Initialize the tokenizer\n",
    "\n",
    "Step 2: Create dataset\n",
    "\n",
    "Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes\n",
    "during training\n",
    "\n",
    "Step 4: The number of CPU processes to use for preprocessing\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4, \n",
    "\n",
    "This will develop an intuition of how the GPTDatasetV1 class and the\n",
    "create_dataloader_v1 function work together: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cpu\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The first_batch variable contains two tensors: the first tensor stores the input token IDs,\n",
    "and the second tensor stores the target token IDs. \n",
    "\n",
    "Since the max_length is set to 4, each of the two tensors contains 4 token IDs. \n",
    "\n",
    "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least\n",
    "256.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "To illustrate the meaning of stride=1, let's fetch another batch from this dataset: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "If we compare the first with the second batch, we can see that the second batch's token\n",
    "IDs are shifted by one position compared to the first batch. \n",
    "\n",
    "For example, the second ID in\n",
    "the first batch's input is 367, which is the first ID of the second batch's input. \n",
    "\n",
    "The stride\n",
    "setting dictates the number of positions the inputs shift across batches, emulating a sliding\n",
    "window approach\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Batch sizes of 1, such as we have sampled from the data loader so far, are useful for\n",
    "illustration purposes. \n",
    "                                                                                 \n",
    "If you have previous experience with deep learning, you may know\n",
    "that small batch sizes require less memory during training but lead to more noisy model\n",
    "updates.\n",
    "\n",
    "Just like in regular deep learning, the batch size is a trade-off and hyperparameter\n",
    "to experiment with when training LLMs.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Before we move on to the two final sections of this chapter that are focused on creating\n",
    "the embedding vectors from the token IDs, let's have a brief look at how we can use the\n",
    "data loader to sample with a batch size greater than 1: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that we increase the stride to 4. This is to utilize the data set fully (we don't skip a\n",
    "single word) but also avoid any overlap between the batches, since more overlap could lead\n",
    "to increased overfitting.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATE TOKEN EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
