{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae923ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "file_path = 'harrypotter.txt'\n",
    "\n",
    "# Open the PDF file in binary mode\n",
    "with open(file_path, \"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8522c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "077fb61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  6290999\n",
      "CONTENTS\n",
      "Harry Potter and the Sorcerer's Stone\n",
      "Harry Potter and the Chamber of Secrets\n",
      "Harry Potter and the P\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of characters: \", len(raw_text))\n",
    "print(raw_text[:109])  # Print the first 99 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3b55c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CONTENTS', 'Harry', 'Potter', 'and', 'the', 'Sorcerer', \"'\", 's', 'Stone', 'Harry', 'Potter', 'and', 'the', 'Chamber', 'of', 'Secrets', 'Harry', 'Potter', 'and', 'the', 'Prisoner', 'of', 'Azkaban', 'Harry', 'Potter', 'and', 'the', 'Goblet', 'of', 'Fire', 'Harry', 'Potter', 'and', 'the', 'Order', 'of', 'the', 'Phoenix', 'Harry', 'Potter']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# test_text = \"Hello world, this is-- a,test.\"\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|-|--|\\s)',raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf2ca5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448745\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524bd97",
   "metadata": {},
   "source": [
    "# Now we create token IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb7a141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '10', '12', '125', '1289', '1296', '12th', '13', '1473', '1492', '15', '150', '160', '1612', '1612–1697', '1637', '1689', '17', '170', '1709', '1722', '1722–1741', '1741–1768', '1749', '1792', '18', '1875', '1890', '1909', '1945']\n",
      "25861\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(all_words[:40])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c58b5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token  in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8ff6ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "('&', 2)\n",
      "(\"'\", 3)\n",
      "('(', 4)\n",
      "(')', 5)\n"
     ]
    }
   ],
   "source": [
    "for i ,item in enumerate (vocab.items()):\n",
    "    print(item)\n",
    "    if (i>=5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dae3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()       \n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afcd1797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2728, 24911, 14799, 6594, 24648, 13617, 16820, 24412, 16171, 6, 25085, 14120, 23293, 14120, 3, 10389, 22431, 14330, 15825, 7065, 24825, 6740, 23209, 20167, 23470, 8754, 14309, 6594, 8672, 13180, 23209, 7580, 8]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"  He was in a very good mood until lunchtime, when he\n",
    "            thought he'd stretch his legs and walk across the road to buy himself a bun\n",
    "            from the bakery.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fda576c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"He was in a very good mood until lunchtime, when he thought he' d stretch his legs and walk across the road to buy himself a bun from the bakery.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ce3f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Babuji, do you like tea?\"\n",
    "# print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a484da",
   "metadata": {},
   "source": [
    "# ADDING SPECIAL CONTEXT TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88229781",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12e76c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25863"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55457cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('“‘the', 27662)\n",
      "('“‘—', 27663)\n",
      "('…', 27664)\n",
      "('<|endoftext|>', 27665)\n",
      "('<|unk|>', 27666)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f9f9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9124c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a316a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2760,\n",
       " 6,\n",
       " 11259,\n",
       " 25521,\n",
       " 15923,\n",
       " 23058,\n",
       " 86,\n",
       " 25861,\n",
       " 2994,\n",
       " 23209,\n",
       " 22680,\n",
       " 25862,\n",
       " 17441,\n",
       " 23209,\n",
       " 17849,\n",
       " 8]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ef94c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit <|unk|> of the palace.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c862b64c",
   "metadata": {},
   "source": [
    "# BYTE PAIR ENCODING (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71b29377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-win_amd64.whl (894 kB)\n",
      "   ---------------------------------------- 0.0/894.0 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/894.0 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 524.3/894.0 kB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 786.4/894.0 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 894.0/894.0 kB 1.2 MB/s eta 0:00:00\n",
      "Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Installing collected packages: urllib3, regex, idna, charset-normalizer, certifi, requests, tiktoken\n",
      "\n",
      "   ---------------------------------------- 0/7 [urllib3]\n",
      "   ---------------------------------------- 0/7 [urllib3]\n",
      "   ---------------------------------------- 0/7 [urllib3]\n",
      "   ----- ---------------------------------- 1/7 [regex]\n",
      "   ----------- ---------------------------- 2/7 [idna]\n",
      "   ----------------- ---------------------- 3/7 [charset-normalizer]\n",
      "   ---------------------------- ----------- 5/7 [requests]\n",
      "   ---------------------------- ----------- 5/7 [requests]\n",
      "   ---------------------------------- ----- 6/7 [tiktoken]\n",
      "   ---------------------------------------- 7/7 [tiktoken]\n",
      "\n",
      "Successfully installed certifi-2025.4.26 charset-normalizer-3.4.1 idna-3.10 regex-2024.11.6 requests-2.32.3 tiktoken-0.9.0 urllib3-2.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9768712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a5d4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8029adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c6049c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04ef14",
   "metadata": {},
   "source": [
    "# DATA SAMPLING WITH SLIDING WINDOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c612a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
